{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LsDBqt8w3qTJ"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NLfByg8Cllug",
    "outputId": "d866d7b8-d620-4269-a55b-c8adce8cdf82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related topics:\n",
      "Machine learning\n",
      "Neural network (machine learning)\n",
      "Attention (machine learning)\n",
      "Quantum machine learning\n",
      "Boosting (machine learning)\n",
      "Transformer (deep learning architecture)\n",
      "Adversarial machine learning\n",
      "Active learning (machine learning)\n",
      "Artificial intelligence\n",
      "Supervised learning\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def search_wikipedia(topic):\n",
    "    search_url = f\"https://en.wikipedia.org/w/api.php?action=query&list=search&srsearch={topic}&format=json\"\n",
    "    response = requests.get(search_url)\n",
    "    search_results = response.json()\n",
    "    return search_results['query']['search']\n",
    "\n",
    "def clean_snippet(snippet):\n",
    "    soup = BeautifulSoup(snippet, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# Example usage\n",
    "topic = \"Machine learning\"\n",
    "search_results = search_wikipedia(topic)\n",
    "\n",
    "# Extract titles from the search results\n",
    "titles = [result['title'] for result in search_results]\n",
    "\n",
    "print(\"Related topics:\")\n",
    "for title in titles:\n",
    "    print(title)\n",
    "\n",
    "# Optional: Convert the titles to a DataFrame if needed\n",
    "df = pd.DataFrame(titles, columns=[\"Title\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rajgo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rajgo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related topics:\n",
      "Machine learning\n",
      "Quantum machine learning\n",
      "Neural network (machine learning)\n",
      "Attention (machine learning)\n",
      "Adversarial machine learning\n",
      "Boosting (machine learning)\n",
      "Transformer (deep learning architecture)\n",
      "Active learning (machine learning)\n",
      "Artificial intelligence\n",
      "Outline of machine learning\n"
     ]
    }
   ],
   "source": [
    "def search_wikipedia(topic):\n",
    "    search_url = f\"https://en.wikipedia.org/w/api.php?action=query&list=search&srsearch={topic}&format=json\"\n",
    "    response = requests.get(search_url)\n",
    "    search_results = response.json()\n",
    "    return search_results['query']['search']\n",
    "\n",
    "def clean_snippet(snippet):\n",
    "    soup = BeautifulSoup(snippet, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    # Remove stop words and non-alphabetic tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Example usage\n",
    "topic = \"Machine learning\"\n",
    "search_results = search_wikipedia(topic)\n",
    "\n",
    "# Extract titles from the search results\n",
    "titles = [result['title'] for result in search_results]\n",
    "\n",
    "# Print related topics (titles)\n",
    "print(\"Related topics:\")\n",
    "for title in titles:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_wikipedia_extract(title, length=\"short\"):\n",
    "    # Define the endpoint\n",
    "    endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # Set the extract length parameters\n",
    "    if length == \"short\":\n",
    "        exchars = 1000  # Limit to approximately one paragraph\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"extracts\",\n",
    "            \"exchars\": exchars,\n",
    "            \"explaintext\": True\n",
    "        }\n",
    "    elif length == \"medium\":\n",
    "        exsentences = 10  # Limit to approximately two paragraphs\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"extracts\",\n",
    "            \"exsentences\": exsentences,\n",
    "            \"explaintext\": True\n",
    "        }\n",
    "    else:\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"extracts\",\n",
    "            \"exintro\": True,\n",
    "            \"explaintext\": True\n",
    "        }\n",
    "\n",
    "    # Make the request to the Wikipedia API\n",
    "    response = requests.get(endpoint, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the page information\n",
    "    pages = data['query']['pages']\n",
    "    page = next(iter(pages.values()))  # Get the first (and only) page\n",
    "\n",
    "    # Extract the extract text\n",
    "    extract = page.get(\"extract\", \"No extract found.\")\n",
    "\n",
    "    return extract\n",
    "\n",
    "def search_wikipedia(topic):\n",
    "    search_url = f\"https://en.wikipedia.org/w/api.php?action=query&list=search&srsearch={topic}&format=json\"\n",
    "    response = requests.get(search_url)\n",
    "    search_results = response.json()\n",
    "    return search_results['query']['search']\n",
    "\n",
    "# Example usage\n",
    "topic = \"Machine learning\"\n",
    "search_results = search_wikipedia(topic)\n",
    "\n",
    "# Extract titles from the search results\n",
    "titles = [result['title'] for result in search_results]\n",
    "\n",
    "# Iterate through each title and get a paragraph (full extract) for each\n",
    "for title in titles:\n",
    "    extract = get_wikipedia_extract(title, length=\"full\")\n",
    "    print(f\"Title: {title}\\nExtract:\\n{extract}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_wikipedia_extract(title, length=\"short\"):\n",
    "    # Define the endpoint\n",
    "    endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # Set the extract length parameters\n",
    "    if length == \"short\":\n",
    "        exchars = 500  # Limit to approximately one paragraph\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"extracts\",\n",
    "            \"exchars\": exchars,\n",
    "            \"explaintext\": True\n",
    "        }\n",
    "    elif length == \"medium\":\n",
    "        exsentences = 5  # Limit to approximately two paragraphs\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"extracts\",\n",
    "            \"exsentences\": exsentences,\n",
    "            \"explaintext\": True\n",
    "        }\n",
    "    else:\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"extracts\",\n",
    "            \"exintro\": True,\n",
    "            \"explaintext\": True\n",
    "        }\n",
    "\n",
    "    # Make the request to the Wikipedia API\n",
    "    response = requests.get(endpoint, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the page information\n",
    "    pages = data['query']['pages']\n",
    "    page = next(iter(pages.values()))  # Get the first (and only) page\n",
    "\n",
    "    # Extract the extract text\n",
    "    extract = page.get(\"extract\", \"No extract found.\")\n",
    "\n",
    "    return extract\n",
    "\n",
    "# Example usage\n",
    "title = \"Machine learning\"\n",
    "length = \"full\"  # Options: \"short\", \"medium\", \"full\"\n",
    "extract = get_wikipedia_extract(title, length)\n",
    "print(extract)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0Suyw1zuxCd"
   },
   "source": [
    "# Model - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h_VYImzx2Id2",
    "outputId": "745a2d75-53b8-4d66-e0cc-6ce0a084b76a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the pirates of carribian written by jack.?\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def convert_to_question(sentence):\n",
    "    # List of auxiliary verbs to identify questions\n",
    "    question_words = [\"is\", \"are\", \"am\", \"was\", \"were\", \"can\", \"could\", \"shall\", \"should\", \"will\", \"would\", \"do\", \"does\", \"did\", \"have\", \"has\", \"had\", \"may\", \"might\", \"must\"]\n",
    "\n",
    "    # Split the sentence into words\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Regex to identify verbs\n",
    "    verb_regex = re.compile(r\"\\bis\\b|\\bare\\b|\\bam\\b|\\bwas\\b|\\bwere\\b|\\bcan\\b|\\bcould\\b|\\bshall\\b|\\bshould\\b|\\bwill\\b|\\bwould\\b|\\bdo\\b|\\bdoes\\b|\\bdid\\b|\\bhave\\b|\\bhas\\b|\\bhad\\b|\\bmay\\b|\\bmight\\b|\\bmust\\b\")\n",
    "\n",
    "    # Find the first auxiliary verb in the sentence\n",
    "    for i, word in enumerate(words):\n",
    "        if verb_regex.match(word.lower()):\n",
    "            # Ensure we handle 'have' correctly\n",
    "            if word.lower() == \"have\" and i != 0:\n",
    "                question_sentence = \"Do \" + \" \".join(words[:i] + [words[i]] + words[i+1:]) + \"?\"\n",
    "            else:\n",
    "                # Move the auxiliary verb to the front\n",
    "                question_sentence = word + \" \" + \" \".join(words[:i] + words[i+1:]) + \"?\"\n",
    "            return question_sentence.capitalize()\n",
    "\n",
    "    # Handle sentences without auxiliary verbs by adding 'do'\n",
    "    if len(words) > 1:\n",
    "        return \"Do \" + sentence + \"?\"\n",
    "    else:\n",
    "        return sentence + \"?\"\n",
    "\n",
    "def paragraph_to_questions(paragraph):\n",
    "    # Split the paragraph into sentences\n",
    "    sentences = re.split(r'(?<=[.!?]) +', paragraph)\n",
    "    questions = [convert_to_question(sentence.strip()) for sentence in sentences if sentence.strip()]\n",
    "    return questions\n",
    "\n",
    "# Example input paragraph\n",
    "paragraph = \"\"\"\n",
    "The pirates of carribian is written by jack.\n",
    "\"\"\"\n",
    "\n",
    "# Generate questions\n",
    "questions = paragraph_to_questions(paragraph)\n",
    "for question in questions:\n",
    "    print(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 Manual MCQ generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mcqs(text, num_questions=5):\n",
    "    # text = clean_text(text)\n",
    "    if text is None:\n",
    "        return []\n",
    "\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract sentences from the text\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "    # Ensure that the number of questions does not exceed the number of sentences\n",
    "    num_questions = min(num_questions, len(sentences))\n",
    "\n",
    "    # Randomly select sentences to form questions\n",
    "    selected_sentences = random.sample(sentences, num_questions)\n",
    "\n",
    "    # Initialize list to store generated MCQs\n",
    "    mcqs = []\n",
    "\n",
    "    # Generate MCQs for each selected sentence\n",
    "    for sentence in selected_sentences:\n",
    "        # Process the sentence with spaCy\n",
    "        sent_doc = nlp(sentence)\n",
    "\n",
    "        # Extract entities (nouns) from the sentence\n",
    "        nouns = [token.text for token in sent_doc if token.pos_ == \"NOUN\"]\n",
    "\n",
    "        # Ensure there are enough nouns to generate MCQs\n",
    "        if len(nouns) < 2:\n",
    "            continue\n",
    "\n",
    "        # Count the occurrence of each noun\n",
    "        noun_counts = Counter(nouns)\n",
    "\n",
    "        # Select the most common noun as the subject of the question\n",
    "        if noun_counts:\n",
    "            subject = noun_counts.most_common(1)[0][0]\n",
    "\n",
    "            # Generate the question stem\n",
    "            question_stem = sentence.replace(subject, \"______\")\n",
    "\n",
    "            # Generate answer choices\n",
    "            answer_choices = [subject]\n",
    "\n",
    "            # Add some random words from the text as distractors\n",
    "            distractors = list(set(nouns) - {subject})\n",
    "\n",
    "            # Ensure there are at least three distractors\n",
    "            while len(distractors) < 3:\n",
    "                distractors.append(\"[Distractor]\")  # Placeholder for missing distractors\n",
    "\n",
    "            random.shuffle(distractors)\n",
    "            for distractor in distractors[:3]:\n",
    "                answer_choices.append(distractor)\n",
    "\n",
    "            # Shuffle the answer choices\n",
    "            random.shuffle(answer_choices)\n",
    "\n",
    "            # Append the generated MCQ to the list\n",
    "            correct_answer = chr(64 + answer_choices.index(subject) + 1)  # Convert index to letter\n",
    "            mcqs.append((question_stem, answer_choices, correct_answer))\n",
    "\n",
    "    return mcqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_text = \"\"\"\n",
    "The universe is vast and filled with mysteries that continue to captivate scientists and astronomers alike. From the depths of space to the farthest reaches of distant galaxies, the cosmos holds countless wonders waiting to be explored.\n",
    "\n",
    "One of the fundamental concepts in astrophysics is the Big Bang theory, which posits that the universe originated from a singular, infinitely dense point nearly 13.8 billion years ago. Over time, the universe expanded and cooled, giving rise to the formation of galaxies, stars, and planets.\n",
    "\n",
    "Galaxies are immense systems containing billions or even trillions of stars, as well as various types of interstellar matter such as gas, dust, and dark matter. The Milky Way, our home galaxy, is a spiral galaxy containing hundreds of billions of stars, including our own Sun.\n",
    "\n",
    "Stars are the celestial objects that shine brightly in the night sky, fueled by nuclear fusion reactions occurring in their cores. They come in a variety of sizes, colors, and temperatures, with some stars being much larger and hotter than others. The life cycle of a star depends on its mass, with massive stars undergoing supernova explosions at the end of their lives, while smaller stars like our Sun eventually evolve into white dwarfs.\n",
    "\n",
    "Planets orbit stars and come in different types, including terrestrial planets like Earth, gas giants like Jupiter, and icy worlds like Neptune. In our solar system, eight planets revolve around the Sun, each with its own unique characteristics and features.\n",
    "\n",
    "Space exploration has allowed humanity to venture beyond Earth and explore the cosmos firsthand. Missions to the Moon, Mars, and beyond have expanded our understanding of the universe and laid the groundwork for future exploration and colonization of other worlds.\n",
    "\n",
    "The search for extraterrestrial life is a central focus of space exploration, driven by the desire to uncover whether life exists beyond Earth. Scientists study the conditions on other planets and moons in our solar system, as well as exoplanets orbiting distant stars, in the hope of finding signs of life elsewhere in the universe.\n",
    "\n",
    "The study of black holes, mysterious regions of spacetime where gravity is so strong that nothing, not even light, can escape, is another area of active research in astrophysics. Black holes come in various sizes, from stellar-mass black holes formed from the collapse of massive stars to supermassive black holes that lurk at the centers of galaxies.\n",
    "\n",
    "Cosmology, the scientific study of the origin, evolution, and eventual fate of the universe, seeks to answer some of the most profound questions about our existence. By analyzing cosmic microwave background radiation, the distribution of galaxies, and the structure of the universe on the largest scales, cosmologists aim to unravel the mysteries of the cosmos and our place within it.\n",
    "\n",
    "\"\"\"\n",
    "mcqs = generate_mcqs(tech_text, num_questions=10)  # Pass the selected number of questions\n",
    "# Ensure each MCQ is formatted correctly as (question_stem, answer_choices, correct_answer)\n",
    "mcqs_with_index = [(i + 1, mcq) for i, mcq in enumerate(mcqs)]\n",
    "\n",
    "for question in mcqs_with_index:\n",
    "    print(\"Question\", question[0], \":\", question[1][0])\n",
    "    print(\"Options:\")\n",
    "    options = question[1][1]\n",
    "    for i, option in enumerate(options):\n",
    "        print(f\"{chr(97 + i)}) {option}\")\n",
    "    print(\"Correct Answer:\", question[1][2])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKypuhga5Osf"
   },
   "source": [
    "# Model - 3 Using llama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1jhBiE25m6n"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Check if GPU is available\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(\"GPU is available and will be used.\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"GPU is not available, using CPU.\")\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Read the PDF and extract text from all pages\n",
    "reader = PdfReader(\"industry.pdf\")\n",
    "\n",
    "all_text = \"\"\n",
    "for page in reader.pages:\n",
    "    all_text += page.extract_text()\n",
    "\n",
    "# Initialize and invoke the LLM\n",
    "llm = Ollama(model='llama3', device=device)  # Assuming Ollama supports GPU\n",
    "query = f\"{all_text}. From this given text, create 5 multiple choice questions\"\n",
    "response = llm.invoke(query)\n",
    "\n",
    "# Stop the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Print the response and the elapsed time\n",
    "print(response)\n",
    "print(f\"Time taken to execute: {elapsed_time} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
